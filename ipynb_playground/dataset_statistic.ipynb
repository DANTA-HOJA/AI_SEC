{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_list = []\n",
    "# stdev_list = []\n",
    "# fish_dsnames = list(Counter(df_train[\"parent (dsname)\"]).keys())\n",
    "\n",
    "# for fish_dsname in fish_dsnames:\n",
    "    \n",
    "#     df_tmp = df_train[df_train[\"parent (dsname)\"] == fish_dsname]\n",
    "#     img_batch = None\n",
    "    \n",
    "#     for img_path in df_tmp[\"path\"]:\n",
    "        \n",
    "#         img: np.ndarray = cv2.imread(img_path)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = img[None, :]\n",
    "        \n",
    "#         if img_batch is None:\n",
    "#             img_batch = deepcopy(img)\n",
    "#         else:\n",
    "#             img_batch = np.append(img_batch, img, axis=0)\n",
    "        \n",
    "#     mean_list.append(np.mean(img_batch, axis=(0, 1, 2)))\n",
    "#     stdev_list.append(np.std(img_batch, axis=(0, 1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_list2array(target_list):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     tmp_array = None\n",
    "#     for value in target_list:\n",
    "#         value = value[None, :]\n",
    "#         if tmp_array is None:\n",
    "#             tmp_array = deepcopy(value)\n",
    "#         else:\n",
    "#             tmp_array = np.append(tmp_array, value, axis=0)\n",
    "\n",
    "#     return tmp_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YuDe, WenWei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_var(df, channel):\n",
    "    x_sum = 0\n",
    "    x_sq_sum = 0\n",
    "    n = 0\n",
    "    \n",
    "    for path in df[\"path\"]:\n",
    "        n += 1\n",
    "        new_img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)[:, :, channel]\n",
    "        f = new_img.shape[0] * new_img.shape[1]\n",
    "        x_sum += np.sum(new_img) / (n * f)\n",
    "        x_sq_sum += np.sum(np.power(new_img, 2))\n",
    "\n",
    "    print(x_sum, x_sq_sum, x_sq_sum - np.power(x_sum, 2) / (n * f))\n",
    "    return np.sqrt(x_sq_sum - np.power(x_sum, 2) / (n * f)) / (n * f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welford's online algorithm [(Reference)]\n",
    "\n",
    "[(Reference)]: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a new value new_value, compute the new count, new mean, the new M2.\n",
    "# mean accumulates the mean of the entire dataset\n",
    "# M2 aggregates the squared distance from the mean\n",
    "# count aggregates the number of samples seen so far\n",
    "def update(existing_aggregate, new_value) -> tuple[int, float, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    (count, mean, M2) = existing_aggregate\n",
    "    count += 1\n",
    "    delta = new_value - mean\n",
    "    mean += delta / count\n",
    "    delta2 = new_value - mean\n",
    "    M2 += delta * delta2\n",
    "    return (count, mean, M2)\n",
    "\n",
    "# Retrieve the mean, variance and sample variance from an aggregate\n",
    "def finalize(existing_aggregate) -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    (count, mean, M2) = existing_aggregate\n",
    "    if count < 2:\n",
    "        return float(\"nan\")\n",
    "    else:\n",
    "        (mean, variance, sample_variance) = (mean, M2 / count, M2 / (count - 1))\n",
    "        return (mean, variance, sample_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welford_online_algo(df:pd.DataFrame, channel:str) -> tuple[int, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if channel not in [\"R\", \"G\", \"B\"]: raise ValueError(\"Channel accept 'R', 'G', 'B' only\")\n",
    "    channel_str2int = {\"R\": 0, \"G\": 1, \"B\": 2}\n",
    "    channel_i = channel_str2int[channel.upper()]\n",
    "\n",
    "    existing_aggregate = (0.0, 0.0, 0.0) # (count, mean, M2)\n",
    "    \n",
    "    with tqdm(total=len(df[\"path\"]), desc=f\"channel_{channel} \") as pbar:\n",
    "        for path in df[\"path\"]:\n",
    "            new_img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)[:, :, channel_i]\n",
    "            for pixel in new_img.flatten():\n",
    "                existing_aggregate = update(existing_aggregate, pixel)\n",
    "            pbar.update(1)\n",
    "            pbar.refresh()\n",
    "    \n",
    "    count, mean, M2 = existing_aggregate\n",
    "    mean, variance, sample_variance = finalize(existing_aggregate)\n",
    "    stdev = np.sqrt(variance)\n",
    "\n",
    "    print(f\"Channel: {channel}\")\n",
    "    print(f\"Total mean: {mean}\")\n",
    "    print(f\"Total standard deviation: {stdev}\")\n",
    "    \n",
    "    return (count, mean, variance, sample_variance, stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess Funtions ( generating `single row` in dataframe )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset_identity_string(dataset_xlsx_path: str):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    path_split: list[str] = dataset_xlsx_path.split(os.sep)\n",
    "    palmskin_cnt_num: str = path_split[6].split(\"_\")[-1] # i[num]\n",
    "    palmskin_alias: str = path_split[7] # e.g. 'RGB_direct_max_zproj'\n",
    "    xlsx_name_split: list[str] = os.path.splitext(path_split[-1])[0].split(\"_\") # e.g. ['DS', 'SURF3C', 'CRPS256', 'SF14', 'INT30', 'DRP45']\n",
    "    \n",
    "    return (palmskin_cnt_num, palmskin_alias, xlsx_name_split[2], xlsx_name_split[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_single_statistic_df(dataset_xlsx_path:str, dataset:str, channel:str,\n",
    "                            count:int, mean:float, variance:float, sample_variance:float, stdev:float):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    (palmskin_cnt_num, palmskin_alias, \\\n",
    "        crop_size, shift_region) = gen_dataset_identity_string(dataset_xlsx_path)\n",
    "    \n",
    "    tmp_dict: dict = {}\n",
    "    \n",
    "    tmp_dict[\"identity\"]: str = palmskin_cnt_num\n",
    "    tmp_dict[\"palmskin_alias\"]: str = palmskin_alias\n",
    "    tmp_dict[\"crop_size\"]: str = crop_size\n",
    "    tmp_dict[\"shift_region\"]: str = shift_region\n",
    "    tmp_dict[\"dataset\"]: str = dataset\n",
    "    tmp_dict[\"channel\"]: str = channel\n",
    "    tmp_dict[\"count\"]: int = count\n",
    "    tmp_dict[\"mean\"]: float = mean\n",
    "    tmp_dict[\"variance\"]: float = variance\n",
    "    tmp_dict[\"sample_variance\"]: float = sample_variance\n",
    "    tmp_dict[\"stdev\"]: float = stdev\n",
    "    tmp_df = pd.DataFrame(tmp_dict, index=[0]) # convert `Dict` to `DataFrame`\n",
    "    \n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_xlsx_path: str = r\"/home/rime97410000/ZebraFish_DB/{Dataset}_Cropped_v2/SEED_2022/{20231030_del_day7_8}_Academia_Sinica_i631/RGB_direct_max_zproj/KMeansORIG_RND2022/DS_SURF3C_CRPS256_SF14_INT30_DRP45.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(dataset_xlsx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df # display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dataset_identity_string(dataset_xlsx_path) # fn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_single_statistic_df(dataset_xlsx_path, \"train\", \"R\", 20, 1.0, 2.0, 1.9, 1.5) # fn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict: dict[str, pd.DataFrame] = {}\n",
    "df_dict[\"train_all\"]      = df[(df[\"dataset\"] == \"train\")]\n",
    "df_dict[\"train_preserve\"] = df[(df[\"dataset\"] == \"train\") & (df[\"state\"] == \"preserve\")]\n",
    "df_dict[\"test_all\"]       = df[(df[\"dataset\"] == \"test\")]\n",
    "df_dict[\"test_preserve\"]  = df[(df[\"dataset\"] == \"test\") & (df[\"state\"] == \"preserve\")]\n",
    "\n",
    "[print(f\"{dataset}: {len(df)}\") for dataset, df in df_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"./dataset_statistic_result.csv\")\n",
    "\n",
    "if csv_path.exists():\n",
    "    statistic_df = pd.read_csv(csv_path, encoding='utf_8')\n",
    "else:\n",
    "    statistic_df = None\n",
    "\n",
    "statistic_df # display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, df in df_dict.items():\n",
    "\n",
    "    for channel in [\"R\", \"G\", \"B\"]:\n",
    "        \n",
    "        tmp_tuple = welford_online_algo(df, channel) # (count, mean, variance, sample_variance, stdev)\n",
    "        tmp_df = gen_single_statistic_df(dataset_xlsx_path, dataset, channel, *tmp_tuple)\n",
    "        \n",
    "        if statistic_df is None:\n",
    "            statistic_df = deepcopy(tmp_df)\n",
    "        else:\n",
    "            statistic_df = pd.concat([statistic_df, tmp_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_df # display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_df.to_csv(\"dataset_statistic_result.csv\", encoding='utf_8_sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebrafish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
