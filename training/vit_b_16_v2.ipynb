{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for `Training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import shutil\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "import json\n",
    "import toml\n",
    "import tomlkit\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import imgaug as ia\n",
    "\n",
    "abs_module_path = Path(\"./../modules/\").resolve()\n",
    "if (abs_module_path.exists()) and (str(abs_module_path) not in sys.path):\n",
    "    sys.path.append(str(abs_module_path)) # add path to scan customized module\n",
    "\n",
    "from logger import init_logger\n",
    "from fileop import create_new_dir\n",
    "from dl_utils import caulculate_metrics, save_model, plot_training_trend, \\\n",
    "                     compose_transform, save_training_logs \n",
    "from dl.utils import set_gpu, gen_class2num_dict, get_fish_path, get_fish_class, \\\n",
    "                     calculate_class_weight, rename_training_dir\n",
    "from dl.ImageDataset import ImgDataset_v2\n",
    "from misc.Timer import Timer\n",
    "from plt_show import plot_in_rgb # server can't use `cv.imshow()`\n",
    "\n",
    "config_dir = Path( \"./../Config/\" ).resolve()\n",
    "\n",
    "# print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_logger = init_logger(r\"Train 'vit_b_16'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `db_path_plan.toml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_dir.joinpath(\"db_path_plan.toml\"), mode=\"r\") as f_reader:\n",
    "    dbpp_config = toml.load(f_reader)\n",
    "\n",
    "db_root = Path(dbpp_config[\"root\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `(TrainModel)_vit_b_16.toml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"(TrainModel)_vit_b_16.toml\"\n",
    "\n",
    "with open(config_dir.joinpath(config_name), mode=\"r\") as f_reader:\n",
    "    config = toml.load(f_reader)\n",
    "\n",
    "# dataset\n",
    "dataset_name             = config[\"dataset\"][\"name\"]\n",
    "dataset_result_alias     = config[\"dataset\"][\"result_alias\"]\n",
    "dataset_gen_method       = config[\"dataset\"][\"gen_method\"]\n",
    "dataset_classif_strategy = config[\"dataset\"][\"classif_strategy\"]\n",
    "dataset_param_name       = config[\"dataset\"][\"param_name\"]\n",
    "\n",
    "# model\n",
    "model_name         = config[\"model\"][\"model_name\"]\n",
    "pretrain_weights   = config[\"model\"][\"pretrain_weights\"]\n",
    "\n",
    "# train_opts\n",
    "train_ratio  = config[\"train_opts\"][\"train_ratio\"]\n",
    "rand_seed    = config[\"train_opts\"][\"random_seed\"]\n",
    "epochs       = config[\"train_opts\"][\"epochs\"]\n",
    "batch_size   = config[\"train_opts\"][\"batch_size\"]\n",
    "\n",
    "# train_opts.optimizer\n",
    "lr           = config[\"train_opts\"][\"optimizer\"][\"learning_rate\"]\n",
    "weight_decay = config[\"train_opts\"][\"optimizer\"][\"weight_decay\"]\n",
    "\n",
    "# train_opts.lr_schedular\n",
    "use_lr_schedular   = config[\"train_opts\"][\"lr_schedular\"][\"enable\"]\n",
    "lr_schedular_step  = config[\"train_opts\"][\"lr_schedular\"][\"step\"]\n",
    "lr_schedular_gamma = config[\"train_opts\"][\"lr_schedular\"][\"gamma\"]\n",
    "\n",
    "# train_opts.earlystop\n",
    "enable_earlystop = config[\"train_opts\"][\"earlystop\"][\"enable\"]\n",
    "max_no_improved  = config[\"train_opts\"][\"earlystop\"][\"max_no_improved\"]\n",
    "\n",
    "# train_opts.data\n",
    "use_hsv               = config[\"train_opts\"][\"data\"][\"use_hsv\"]\n",
    "aug_on_fly            = config[\"train_opts\"][\"data\"][\"aug_on_fly\"]\n",
    "forcing_balance       = config[\"train_opts\"][\"data\"][\"forcing_balance\"]\n",
    "forcing_sample_amount = config[\"train_opts\"][\"data\"][\"forcing_sample_amount\"]\n",
    "if aug_on_fly and forcing_balance:\n",
    "    raise ValueError(\"'aug_on_fly' and 'forcing_balance' can only set one to True at a time\")\n",
    "\n",
    "# train_opts.debug_mode\n",
    "debug_mode        = config[\"train_opts\"][\"debug_mode\"][\"enable\"]\n",
    "debug_rand_select = config[\"train_opts\"][\"debug_mode\"][\"rand_select\"]\n",
    "\n",
    "# train_opts.cuda\n",
    "cuda_idx = config[\"train_opts\"][\"cuda\"][\"index\"]\n",
    "use_amp  = config[\"train_opts\"][\"cuda\"][\"use_amp\"]\n",
    "\n",
    "# train_opts.cpu.multiworker\n",
    "num_workers = config[\"train_opts\"][\"cpu\"][\"num_workers\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate `path_vars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cropped_root = db_root.joinpath(dbpp_config[\"dataset_cropped_v2\"])\n",
    "model_cmd_root = db_root.joinpath(dbpp_config[\"model_cmd\"])\n",
    "\n",
    "dataset_xlsx_path = dataset_cropped_root.joinpath(dataset_name, dataset_result_alias, dataset_gen_method, \n",
    "                                                      dataset_classif_strategy, f\"{dataset_param_name}.xlsx\")\n",
    "assert dataset_xlsx_path.exists(), f\"Can't find `dataset_xlsx`: '{dataset_xlsx_path}'\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `'dataset_xlsx'` as DataFrame ( pandas )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_xlsx: pd.DataFrame = pd.read_excel(dataset_xlsx_path, engine = 'openpyxl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: split `train/valid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df_dataset_xlsx[(df_dataset_xlsx['dataset'] == \"train\") & \n",
    "                                (df_dataset_xlsx['state'] == \"preserve\")]\n",
    "train_name_list = []\n",
    "valid_name_list = []\n",
    "train_class_counts = {}\n",
    "\n",
    "for cls in [\"L\", \"M\", \"S\"]:\n",
    "\n",
    "    df = df_training[(df_training['class'] == cls)]\n",
    "    train = df.sample(frac=train_ratio, replace=False, random_state=2022)\n",
    "    valid = df[~df.index.isin(train.index)]\n",
    "    train_name_list.extend(list(train[\"image_name\"]))\n",
    "    valid_name_list.extend(list(valid[\"image_name\"]))\n",
    "\n",
    "    train_d = train[(train['cut_section'] == \"D\")]\n",
    "    train_u = train[(train['cut_section'] == \"U\")]\n",
    "    valid_d = valid[(valid['cut_section'] == \"D\")]\n",
    "    valid_u = valid[(valid['cut_section'] == \"U\")]\n",
    "\n",
    "    print(f\"train_d: {len(train_d)}, train_u: {len(train_u)}\")\n",
    "    print(f\"valid_d: {len(valid_d)}, valid_u: {len(valid_u)}\")\n",
    "    \n",
    "    train_class_counts[cls] = len(train)\n",
    "    \n",
    "print(len(train_name_list), len(valid_name_list), \"\\n\")\n",
    "print(train_class_counts)\n",
    "\n",
    "# train_d: 347, train_u: 333\n",
    "# valid_d: 78, valid_u: 92\n",
    "# train_d: 265, train_u: 263\n",
    "# valid_d: 65, valid_u: 67\n",
    "# train_d: 382, train_u: 378\n",
    "# valid_d: 93, valid_u: 97\n",
    "# 1968 492 \n",
    "\n",
    "# {'L': 680, 'M': 528, 'S': 760}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = get_fish_path(train_name_list[-1], df_training)\n",
    "training_logger.info(f\"Read Test: '{img_path}'\")\n",
    "plot_in_rgb(str(img_path), (512, 512))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class2num_dict = {'L': 0, 'M': 1, 'S': 2}\n",
    "# transform = compose_transform()\n",
    "# train_set = ImgDataset_v2(train_name_list, df_dataset_xlsx, class_mapper=class2num_dict, resize=(224, 224),\n",
    "#                           use_hsv=use_hsv, transform=transform, logger=training_logger)\n",
    "# train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# for data in train_dataloader:\n",
    "#     x_train, y_train, crop_name_batch = data\n",
    "    \n",
    "#     x_train = x_train[0]\n",
    "#     y_train = y_train[0]\n",
    "#     crop_name_batch = crop_name_batch[0]\n",
    "    \n",
    "#     x_train = np.moveaxis(x_train.cpu().numpy(), 0, -1)\n",
    "#     path = get_fish_path(crop_name_batch, df_dataset_xlsx)\n",
    "#     cls = get_fish_class(crop_name_batch, df_dataset_xlsx)\n",
    "    \n",
    "#     print(crop_name_batch, cls, y_train, f'{path}')\n",
    "#     plot_in_rgb(str(path), (512, 512))\n",
    "#     plt.imshow(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU\n",
    "device = set_gpu(cuda_idx, training_logger)\n",
    "\n",
    "\n",
    "# Create save model directory\n",
    "time_stamp = datetime.now().strftime('%Y%m%d_%H_%M_%S')\n",
    "save_dir = model_cmd_root.joinpath(f\"Training_{time_stamp}\")\n",
    "create_new_dir(save_dir)\n",
    "with open(save_dir.joinpath(\"train_config.toml\"), mode=\"w\") as f_writer:\n",
    "    tomlkit.dump(config, f_writer)\n",
    "\n",
    "\n",
    "# Set 'np.random.seed', 'torch.manual_seed'\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed) # Dataloader shuffle consistency\n",
    "ia.seed(rand_seed) # To get consistent augmentations ( 'imgaug' package )\n",
    "\n",
    "\n",
    "# Create transform and Set 'rand_seed' if 'aug_on_fly' is True\n",
    "if aug_on_fly: transform = compose_transform()\n",
    "else: transform = None\n",
    "\n",
    "\n",
    "# Scan classes to create 'class_mapper'\n",
    "num2class_list = sorted(Counter(df_dataset_xlsx[\"class\"]).keys())\n",
    "class2num_dict = gen_class2num_dict(num2class_list)\n",
    "training_logger.info(f\"num2class_list = {num2class_list}, class2num_dict = {class2num_dict}\")\n",
    "# CLI >> num2class_list = ['L', 'M', 'S'], class2num_dict = {'L': 0, 'M': 1, 'S': 2}\n",
    "\n",
    "\n",
    "# Split train, valid set\n",
    "#  TODO:  forcing_balance\n",
    "\n",
    "df_training = df_dataset_xlsx[(df_dataset_xlsx['dataset'] == \"train\") & \n",
    "                                (df_dataset_xlsx['state'] == \"preserve\")]\n",
    "train_name_list = []\n",
    "valid_name_list = []\n",
    "train_class_counts = {}\n",
    "\n",
    "for cls in num2class_list:\n",
    "\n",
    "    df = df_training[(df_training['class'] == cls)]\n",
    "    train = df.sample(frac=train_ratio, replace=False, random_state=2022)\n",
    "    valid = df[~df.index.isin(train.index)]\n",
    "    train_name_list.extend(list(train[\"image_name\"]))\n",
    "    valid_name_list.extend(list(valid[\"image_name\"]))\n",
    "\n",
    "    train_d = train[(train['cut_section'] == \"D\")]\n",
    "    train_u = train[(train['cut_section'] == \"U\")]\n",
    "    valid_d = valid[(valid['cut_section'] == \"D\")]\n",
    "    valid_u = valid[(valid['cut_section'] == \"U\")]\n",
    "\n",
    "    training_logger.info(f\"{cls}: [ train_d: {len(train_d)}, train_u: {len(train_u)} ] \"\n",
    "                                f\"[ valid_d: {len(valid_d)}, valid_u: {len(valid_u)} ]\")\n",
    "    \n",
    "    train_class_counts[cls] = len(train)\n",
    "\n",
    "\n",
    "## save 'training_amount'\n",
    "training_amount = f\"{{ dataset_{len(df_training)} }}_{{ train_{len(train_name_list)} }}_{{ valid_{len(valid_name_list)} }}\"\n",
    "with open(save_dir.joinpath(training_amount), mode=\"w\") as f_writer: pass\n",
    "\n",
    "\n",
    "# Create 'train_set', 'train_dataloader'\n",
    "training_logger.info(f\"train_data ({len(train_name_list)})\")\n",
    "[training_logger.info(f\"{i} : img_path = {train_name_list[i]}\") for i in range(5)]\n",
    "train_set = ImgDataset_v2(train_name_list, df_dataset_xlsx, class_mapper=class2num_dict, resize=(224, 224),\n",
    "                          use_hsv=use_hsv, transform=transform, logger=training_logger)\n",
    "if num_workers > 0:\n",
    "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n",
    "                                  pin_memory=True, num_workers=num_workers, \n",
    "                                  worker_init_fn=seed_worker, generator=g)\n",
    "else: train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "training_logger.info(f\"※ : total train batches: {len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "# Create 'valid_set', 'valid_dataloader'\n",
    "training_logger.info(f\"valid_data ({len(valid_name_list)})\")\n",
    "[training_logger.info(f\"{i} : img_path = {valid_name_list[i]}\") for i in range(5)]\n",
    "valid_set = ImgDataset_v2(valid_name_list, df_dataset_xlsx, class_mapper=class2num_dict, resize=(224, 224),\n",
    "                          use_hsv=use_hsv)\n",
    "if num_workers > 0:\n",
    "    valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, \n",
    "                                  pin_memory=True, num_workers=num_workers)\n",
    "else: valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "training_logger.info(f\"※ : total valid batches: {len(valid_dataloader)}\")\n",
    "\n",
    "\n",
    "# Read test ( debug mode only )\n",
    "if debug_mode:\n",
    "    img_path = get_fish_path(train_name_list[-1], df_training)\n",
    "    training_logger.info(f\"Read Test: '{img_path}'\")\n",
    "    plot_in_rgb(str(img_path), (512, 512))\n",
    "\n",
    "\n",
    "# Create model ( ref: https://github.com/pytorch/vision/issues/7397 )\n",
    "training_logger.info(f\"load model from `torchvision`, model_name: '{model_name}', pretrain_weights: '{pretrain_weights}'\")\n",
    "model = getattr(torchvision.models, model_name)\n",
    "model = model(weights=pretrain_weights)\n",
    "## modify model structure\n",
    "model.heads.head = nn.Linear(in_features=768, out_features=len(class2num_dict), bias=True)\n",
    "model.to(device)\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# Initial 'loss function' and 'optimizer'\n",
    "if aug_on_fly:\n",
    "    # `loss_function` with `class_weight`\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=calculate_class_weight(train_class_counts))\n",
    "else: \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=weight_decay, momentum=0.9)\n",
    "\n",
    "\n",
    "# Initial 'lr_scheduler'\n",
    "if use_lr_schedular: \n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_schedular_step, gamma=lr_schedular_gamma)\n",
    "\n",
    "\n",
    "# Automatic Mixed Precision (amp package)\n",
    "if use_amp: scaler = GradScaler()\n",
    "\n",
    "\n",
    "# Training\n",
    "## training variables\n",
    "training_timer = Timer()\n",
    "train_logs = []\n",
    "valid_logs = []\n",
    "score_key = \"maweavg_f1\"; training_logger.info(\"※ : maweavg_f1 = ( macro_f1 + weighted_f1 ) / 2\")\n",
    "## progress bar\n",
    "pbar_n_epoch = tqdm(total=epochs, desc=f\"Epoch \")\n",
    "pbar_n_train = tqdm(total=len(train_dataloader), desc=\"Train \")\n",
    "pbar_n_valid = tqdm(total=len(valid_dataloader), desc=\"Valid \")\n",
    "## best validation condition\n",
    "best_val_log = { \"Best\": time_stamp, \"epoch\": 0 }\n",
    "best_val_avg_loss = np.inf\n",
    "best_val_f1 = 0.0\n",
    "best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "best_optimizer_state_dict = copy.deepcopy(optimizer.state_dict())\n",
    "## early stop\n",
    "accum_no_improved = 0\n",
    "## exception\n",
    "training_state = None\n",
    "try:\n",
    "    \n",
    "    ## TODO:  recover training\n",
    "    training_timer.start()\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar_n_epoch.desc = f\"Epoch {epoch} \"\n",
    "        pbar_n_epoch.refresh()\n",
    "        pbar_n_train.n = 0\n",
    "        pbar_n_train.refresh()\n",
    "        pbar_n_valid.n = 0\n",
    "        pbar_n_valid.refresh()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Start training\n",
    "        ## reset variables\n",
    "        epoch_train_log = { \"Train\": \"\", \"epoch\": epoch }\n",
    "        pred_list = []\n",
    "        gt_list = []\n",
    "        accum_batch_loss = 0.0\n",
    "        ## set to training mode\n",
    "        model.train()\n",
    "        for data in train_dataloader:\n",
    "            x_train, y_train, crop_name_batch = data\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device) # move to GPU\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad() # clean gradients before each iteration\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    preds = model(x_train)\n",
    "                    loss_value = loss_fn(preds, y_train)\n",
    "                    \n",
    "                scaler.scale(loss_value).backward() # 計算並縮放損失的梯度\n",
    "                scaler.step(optimizer) # 更新模型參數\n",
    "                scaler.update() # 更新縮放因子\n",
    "                \n",
    "            else:\n",
    "                preds = model(x_train)\n",
    "                loss_value = loss_fn(preds, y_train)\n",
    "                \n",
    "                ## update mode_parameters by back_propagation\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            ## extend 'pred_list', 'gt_list'\n",
    "            pred_prob = torch.nn.functional.softmax(preds, dim=1)\n",
    "            _, pred_train = torch.max(pred_prob, 1) # get the highest probability class\n",
    "            pred_list.extend(pred_train.cpu().numpy().tolist()) # conversion flow: Tensor --> ndarray --> list\n",
    "            gt_list.extend(y_train.cpu().numpy().tolist())\n",
    "            ## add current batch loss\n",
    "            accum_batch_loss += loss_value.item() # get value of Tensor\n",
    "            \n",
    "            ## update 'pbar_n_train'\n",
    "            pbar_n_train.update(1)\n",
    "            pbar_n_train.refresh()\n",
    "        \n",
    "        if use_lr_schedular: lr_scheduler.step() # update 'lr' for each epoch\n",
    "        \n",
    "        caulculate_metrics(epoch_train_log, (accum_batch_loss/len(train_dataloader)),\n",
    "                           gt_list, pred_list, class2num_dict)\n",
    "        # print(json.dumps(epoch_train_log, indent=4))\n",
    "        train_logs.append(epoch_train_log)\n",
    "        ## update postfix of 'pbar_n_train'\n",
    "        if use_lr_schedular: pbar_n_train.postfix = (f\" {'{'} Loss: {epoch_train_log['average_loss']}, \"\n",
    "                                                     f\"{score_key}: {epoch_train_log[f'{score_key}']}, \"\n",
    "                                                     f\"lr: {lr_scheduler.get_last_lr()[0]:.0e} {'}'} \")\n",
    "        else: pbar_n_train.postfix = (f\" {'{'} Loss: {epoch_train_log['average_loss']}, \"\n",
    "                                      f\"{score_key}: {epoch_train_log[f'{score_key}']} {'}'} \")\n",
    "        pbar_n_train.refresh()\n",
    "        # End training\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Start validating\n",
    "        ## reset variables\n",
    "        epoch_valid_log = { \"Valid\": \"\", \"epoch\": epoch }\n",
    "        pred_list = []\n",
    "        gt_list = []\n",
    "        accum_batch_loss = 0.0\n",
    "        ## set to evaluation mode\n",
    "        model.eval() \n",
    "        with torch.no_grad(): \n",
    "            for data in valid_dataloader:\n",
    "                x_valid, y_valid, crop_name_batch = data\n",
    "                x_valid, y_valid = x_valid.to(device), y_valid.to(device) # move to GPU\n",
    "                preds = model(x_valid)\n",
    "                loss_value = loss_fn(preds, y_valid)\n",
    "                \n",
    "                ## extend 'pred_list', 'gt_list'\n",
    "                pred_prob = torch.nn.functional.softmax(preds, dim=1)\n",
    "                _, pred_valid = torch.max(pred_prob, 1)\n",
    "                pred_list.extend(pred_valid.cpu().numpy().tolist())\n",
    "                gt_list.extend(y_valid.cpu().numpy().tolist())\n",
    "                ## add current batch loss\n",
    "                accum_batch_loss += loss_value.item()\n",
    "                \n",
    "                ## update 'pbar_n_valid'\n",
    "                pbar_n_valid.update(1)\n",
    "                pbar_n_valid.refresh()\n",
    "\n",
    "        caulculate_metrics(epoch_valid_log, (accum_batch_loss/len(valid_dataloader)),\n",
    "                           gt_list, pred_list, class2num_dict)\n",
    "        # print(json.dumps(epoch_valid_log, indent=4))\n",
    "        valid_logs.append(epoch_valid_log)\n",
    "        ## update postfix of 'pbar_n_valid'\n",
    "        pbar_n_valid.postfix = (f\" {'{'} Loss: {epoch_valid_log['average_loss']}, \"\n",
    "                                f\"{score_key}: {epoch_valid_log[f'{score_key}']} {'}'} \")\n",
    "        pbar_n_valid.refresh()\n",
    "        # End validating\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Check best condition\n",
    "        epoch_valid_log[\"valid_state\"] = \"\"\n",
    "        if epoch_valid_log[f\"{score_key}\"] > best_val_f1:\n",
    "            best_val_f1 = epoch_valid_log[f\"{score_key}\"]\n",
    "            ## update 'best_val_log'\n",
    "            caulculate_metrics(best_val_log, (accum_batch_loss/len(valid_dataloader)),\n",
    "                               gt_list, pred_list, class2num_dict)\n",
    "            epoch_valid_log[\"valid_state\"] = \"☆★☆ BEST_VALIDATION ☆★☆\"\n",
    "            tqdm.write((f\"Epoch: {epoch:0{len(str(epochs))}}, \"\n",
    "                        f\"☆★☆ BEST_VALIDATION ☆★☆, \"\n",
    "                        f\"best_val_avg_loss = {best_val_log['average_loss']}, \"\n",
    "                        f\"best_val_{score_key} = {best_val_log[f'{score_key}']}\"))\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "            best_optimizer_state_dict = copy.deepcopy(optimizer.state_dict())\n",
    "            best_val_log[\"epoch\"] = epoch # put here to make sure all updates of best state has been done.\n",
    "        \n",
    "        # Update figure \n",
    "        plot_training_trend_kwargs = {\n",
    "            \"plt\"        : plt,\n",
    "            \"save_dir\"   : save_dir,\n",
    "            \"loss_key\"   : \"average_loss\",\n",
    "            \"score_key\"  : score_key,\n",
    "            \"train_logs\" : pd.DataFrame(train_logs),\n",
    "            \"valid_logs\" : pd.DataFrame(valid_logs),\n",
    "        }\n",
    "        plot_training_trend(**plot_training_trend_kwargs)\n",
    "        \n",
    "        # Update 'pbar_n_epoch'\n",
    "        pbar_n_epoch.update(1)\n",
    "        pbar_n_epoch.refresh()\n",
    "        \n",
    "        \n",
    "        # Check 'EarlyStop'\n",
    "        epoch_valid_log[\"valid_improve\"] = \"\"\n",
    "        if enable_earlystop:\n",
    "            if epoch_valid_log[\"average_loss\"] < best_val_avg_loss:\n",
    "                best_val_avg_loss = epoch_valid_log[\"average_loss\"]\n",
    "                accum_no_improved = 0\n",
    "            else:\n",
    "                epoch_valid_log[\"valid_improve\"] = \"◎㊣◎ NO_IMPROVED ◎㊣◎\"\n",
    "                accum_no_improved += 1\n",
    "                tqdm.write(f\"Epoch: {epoch:0{len(str(epochs))}}, ◎㊣◎ NO_IMPROVED ◎㊣◎, accum_no_improved = {accum_no_improved}\")\n",
    "                if accum_no_improved == max_no_improved: sys.exit()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    training_state = \"KeyboardInterrupt\"\n",
    "    tqdm.write(\"KeyboardInterrupt\")\n",
    "    \n",
    "except SystemExit:\n",
    "    training_state = \"EarlyStop\"\n",
    "    tqdm.write(\"EarlyStop, exit training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    training_state = \"ExceptionError\"\n",
    "    tqdm.write(traceback.format_exc())\n",
    "    with open(save_dir.joinpath(f\"{{Logs}}_ExceptionError.log\"), mode=\"w\") as f_writer:\n",
    "        f_writer.write(traceback.format_exc())\n",
    "\n",
    "else:\n",
    "    training_state = \"Completed\"\n",
    "    tqdm.write(\"Training Completed\")\n",
    "    \n",
    "finally:\n",
    "\n",
    "    # Close 'progress_bar'\n",
    "    pbar_n_epoch.close()\n",
    "    pbar_n_train.close()\n",
    "    pbar_n_valid.close()\n",
    "    \n",
    "    # Save training consume time\n",
    "    training_timer.stop()\n",
    "    training_timer.calculate_consume_time()\n",
    "    training_timer.save_consume_time(save_dir, desc=\"training time\")\n",
    "    \n",
    "    \n",
    "    if best_val_log[\"epoch\"] > 0: # If `best_val_log[\"epoch\"]` > 0, all of `logs` and `state_dict` are not empty.\n",
    "        \n",
    "        # Save logs (convert to Dataframe)\n",
    "        save_training_logs(save_dir, train_logs, valid_logs, best_val_log)\n",
    "        \n",
    "        # Save model\n",
    "        save_model(\"best\", save_dir, best_model_state_dict, best_optimizer_state_dict, best_val_log)\n",
    "        save_model(\"final\", save_dir, model.state_dict(), optimizer.state_dict(), {\"train\": train_logs, \"valid\": valid_logs})\n",
    "\n",
    "        # Rename 'save_dir'\n",
    "        ## new_name_format = {time_stamp}_{state}_{target_epochs_with_ImgLoadOptions}_{test_f1}\n",
    "        ## state = {EarlyStop, Interrupt, Completed, Tested, etc.}\n",
    "        rename_training_dir(save_dir, time_stamp=time_stamp, state=training_state,\n",
    "                            epochs=valid_logs[-1][\"epoch\"], aug_on_fly=aug_on_fly, use_hsv=use_hsv)\n",
    "        \n",
    "    else: # Delete folder if less than one epoch has been completed.\n",
    "        \n",
    "        training_logger.info(f\"Less than One epoch has been completed, remove directory '{save_dir}' \")\n",
    "        shutil.rmtree(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebrafish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae46fe3be2f97d3a16702042bc6c7abd422dd0bfb5ce5527ad30c3a287e1c756"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
