{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for `Training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import traceback\n",
    "import shutil\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import json\n",
    "import toml\n",
    "import tomlkit\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import imgaug as ia\n",
    "\n",
    "sys.path.append(\"./../modules/\") # add path to scan customized module\n",
    "from logger import init_logger\n",
    "from fileop import create_new_dir\n",
    "from dl_utils import set_gpu, ImgDataset, caulculate_metrics, save_model, plot_training_trend, \\\n",
    "                     compose_transform, calculate_class_weight, get_sortedClassMapper_from_dir, \\\n",
    "                     rename_training_dir, save_training_logs\n",
    "from misc_utils import Timer\n",
    "from plt_show import plot_in_rgb # server can't use `cv.imshow()`\n",
    "\n",
    "# print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_logger = init_logger(r\"Training\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `vit_b_16.toml` ( train_ver )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vit_b_16.toml\", mode=\"r\") as f_reader:\n",
    "    config = toml.load(f_reader)\n",
    "\n",
    "# dataset\n",
    "dataset_root       = os.path.normpath(config[\"dataset\"][\"root\"])\n",
    "dataset_name       = config[\"dataset\"][\"name\"]\n",
    "dataset_gen_method = config[\"dataset\"][\"gen_method\"]\n",
    "dataset_stdev      = config[\"dataset\"][\"stdev\"]\n",
    "dataset_param_name = config[\"dataset\"][\"param_name\"]\n",
    "\n",
    "# model\n",
    "save_dir_root    = config[\"model\"][\"history_root\"]\n",
    "model_name       = config[\"model\"][\"model_name\"]\n",
    "pretrain_weights = config[\"model\"][\"pretrain_weights\"]\n",
    "\n",
    "# train_opts\n",
    "train_ratio  = config[\"train_opts\"][\"train_ratio\"]\n",
    "rand_seed    = config[\"train_opts\"][\"random_seed\"]\n",
    "epochs       = config[\"train_opts\"][\"epochs\"]\n",
    "batch_size   = config[\"train_opts\"][\"batch_size\"]\n",
    "\n",
    "# train_opts.optimizer\n",
    "lr           = config[\"train_opts\"][\"optimizer\"][\"learning_rate\"]\n",
    "weight_decay = config[\"train_opts\"][\"optimizer\"][\"weight_decay\"]\n",
    "\n",
    "# train_opts.lr_schedular\n",
    "use_lr_schedular   = config[\"train_opts\"][\"lr_schedular\"][\"enable\"]\n",
    "lr_schedular_step  = config[\"train_opts\"][\"lr_schedular\"][\"step\"]\n",
    "lr_schedular_gamma = config[\"train_opts\"][\"lr_schedular\"][\"gamma\"]\n",
    "\n",
    "# train_opts.earlystop\n",
    "enable_earlystop = config[\"train_opts\"][\"earlystop\"][\"enable\"]\n",
    "max_no_improved  = config[\"train_opts\"][\"earlystop\"][\"max_no_improved\"]\n",
    "\n",
    "# train_opts.data\n",
    "use_hsv               = config[\"train_opts\"][\"data\"][\"use_hsv\"]\n",
    "aug_on_fly            = config[\"train_opts\"][\"data\"][\"aug_on_fly\"]\n",
    "forcing_balance       = config[\"train_opts\"][\"data\"][\"forcing_balance\"]\n",
    "forcing_sample_amount = config[\"train_opts\"][\"data\"][\"forcing_sample_amount\"]\n",
    "if aug_on_fly and forcing_balance: \n",
    "    raise ValueError(\"'aug_on_fly' and 'forcing_balance' can only set one to True at a time\")\n",
    "\n",
    "# train_opts.debug_mode\n",
    "debug_mode        = config[\"train_opts\"][\"debug_mode\"][\"enable\"]\n",
    "debug_rand_select = config[\"train_opts\"][\"debug_mode\"][\"rand_select\"]\n",
    "\n",
    "# train_opts.cuda\n",
    "cuda_idx = config[\"train_opts\"][\"cuda\"][\"index\"]\n",
    "use_amp  = config[\"train_opts\"][\"cuda\"][\"use_amp\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate `path_vars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = os.path.join(save_dir_root, model_name)\n",
    "dataset_dir = os.path.join(dataset_root, dataset_name, dataset_gen_method, dataset_stdev, dataset_param_name)\n",
    "train_selected_dir = os.path.join(dataset_dir, \"train\", \"selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU\n",
    "device, device_name = set_gpu(cuda_idx)\n",
    "training_logger.info(f\"Using '{device}', device_name = '{device_name}'\")\n",
    "\n",
    "\n",
    "# Create save model directory\n",
    "time_stamp = datetime.now().strftime('%Y%m%d_%H_%M_%S')\n",
    "save_dir = os.path.join(model_save_dir, f\"Training_{time_stamp}\")\n",
    "create_new_dir(save_dir)\n",
    "with open(os.path.normpath(f\"{save_dir}/train_config.toml\"), mode=\"w\") as f_writer:\n",
    "    tomlkit.dump(config, f_writer)\n",
    "\n",
    "\n",
    "# Set 'np.random.seed'\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "\n",
    "# Create transform and Set 'rand_seed' if 'aug_on_fly' is True\n",
    "if aug_on_fly: \n",
    "    ia.seed(rand_seed) # To get consistent augmentations ( 'imgaug' package )\n",
    "    transform = compose_transform()\n",
    "else: transform = None\n",
    "\n",
    "\n",
    "# Scan classes to create 'class_mapper'\n",
    "num2class_list, class2num_dict = get_sortedClassMapper_from_dir(train_selected_dir)\n",
    "training_logger.info(f\"num2class_list = {num2class_list}, class2num_dict = {class2num_dict}\")\n",
    "\n",
    "\n",
    "# Scan tiff\n",
    "dataset_img_dict = { \"all_classes\" : [] }\n",
    "# random sampling from each class with a constant value (forcing balance)\n",
    "if aug_on_fly: dataset_img_dict['all_classes'] = glob(os.path.normpath(f\"{train_selected_dir}/*/*selected*.tiff\"))\n",
    "elif forcing_balance:\n",
    "    for key, value in class2num_dict.items(): # key: class, value: class_idx\n",
    "        dataset_img_dict[key] = glob(os.path.normpath(f\"{train_selected_dir}/{key}/*.tiff\"))\n",
    "        dataset_img_dict[key] = np.random.choice(dataset_img_dict[key], size=forcing_sample_amount, replace=False)\n",
    "        dataset_img_dict['all_classes'].extend(dataset_img_dict[key])\n",
    "else: dataset_img_dict['all_classes'] = glob(os.path.normpath(f\"{train_selected_dir}/*/*.tiff\"))\n",
    "training_logger.info(f\"total = {len(dataset_img_dict['all_classes'])}\")\n",
    "## debug mode: random select [debug_rand_select] images\n",
    "if debug_mode:\n",
    "    dataset_img_dict['all_classes'] = np.random.choice(dataset_img_dict['all_classes'], size=debug_rand_select, replace=False)\n",
    "    training_logger.info(f\"Debug mode, only select first {len(dataset_img_dict['all_classes'])}\")\n",
    "\n",
    "\n",
    "# Split train, valid dataset\n",
    "train_img_list, valid_img_list = train_test_split(dataset_img_dict['all_classes'], random_state=rand_seed, train_size=train_ratio)\n",
    "## save 'training_amount'\n",
    "training_amount = f\"{{ dataset_{len(dataset_img_dict['all_classes'])} }}_{{ train_{len(train_img_list)} }}_{{ valid_{len(valid_img_list)} }}\"\n",
    "with open(os.path.normpath(f\"{save_dir}/{training_amount}\"), mode=\"w\") as f_writer: pass\n",
    "\n",
    "\n",
    "# Create 'train_set', 'train_dataloader'\n",
    "training_logger.info(f\"train_data ({len(train_img_list)})\")\n",
    "[training_logger.info(f\"{i} : img_path = {train_img_list[i]}\") for i in range(5)]\n",
    "train_set = ImgDataset(train_img_list, class_mapper=class2num_dict, resize=(224, 224), \n",
    "                       use_hsv=use_hsv, transform=transform, logger=training_logger)\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True) # TODO:  Dataloader shuffle consistency\n",
    "training_logger.info(f\"※ : total train batches: {len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "# Create 'valid_set', 'valid_dataloader'\n",
    "training_logger.info(f\"valid_data ({len(valid_img_list)})\")\n",
    "[training_logger.info(f\"{i} : img_path = {valid_img_list[i]}\") for i in range(5)]\n",
    "valid_set = ImgDataset(valid_img_list, class_mapper=class2num_dict, resize=(224, 224), \n",
    "                       use_hsv=use_hsv)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "training_logger.info(f\"※ : total valid batches: {len(valid_dataloader)}\")\n",
    "\n",
    "\n",
    "# Read test ( debug mode only )\n",
    "if debug_mode:\n",
    "    training_logger.info(f\"Read Test: {train_img_list[-1]}\")\n",
    "    plot_in_rgb(train_img_list[-1], (512, 512))\n",
    "\n",
    "\n",
    "# Create model ( ref: https://github.com/pytorch/vision/issues/7397 )\n",
    "training_logger.info(f\"load model from `torchvision`, model_name: '{model_name}', pretrain_weights: '{pretrain_weights}'\")\n",
    "model = getattr(torchvision.models, model_name)\n",
    "model = model(weights=pretrain_weights)\n",
    "## modify model structure\n",
    "model.heads.head = nn.Linear(in_features=768, out_features=len(class2num_dict), bias=True)\n",
    "model.to(device)\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# Initial 'loss function' and 'optimizer'\n",
    "if aug_on_fly:\n",
    "    logs_path = os.path.join(dataset_dir, r\"{Logs}_train_selected_summary.log\")\n",
    "    with open(logs_path, 'r') as f_writer: class_counts: Dict[str, int] = json.load(f_writer)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=calculate_class_weight(class_counts)) # apply 'class_weight'\n",
    "else: \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=weight_decay, momentum=0.9)\n",
    "\n",
    "\n",
    "# Initial 'lr_scheduler'\n",
    "if use_lr_schedular: lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_schedular_step, gamma=lr_schedular_gamma)\n",
    "\n",
    "\n",
    "# Automatic Mixed Precision (amp package)\n",
    "if use_amp: scaler = GradScaler()\n",
    "\n",
    "\n",
    "# Training\n",
    "## training variables\n",
    "training_timer = Timer()\n",
    "train_logs = []\n",
    "valid_logs = []\n",
    "## progress bar\n",
    "pbar_n_epoch = tqdm(total=epochs, desc=f\"Epoch \")\n",
    "pbar_n_train = tqdm(total=len(train_dataloader), desc=\"Train \")\n",
    "pbar_n_valid = tqdm(total=len(valid_dataloader), desc=\"Valid \")\n",
    "## best validation condition\n",
    "best_val_log = { \"Best\": time_stamp, \"epoch\": 0 }\n",
    "best_val_avg_loss = np.inf\n",
    "best_val_weighted_f1 = 0.0\n",
    "best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "best_optimizer_state_dict = copy.deepcopy(optimizer.state_dict())\n",
    "## early stop\n",
    "accum_no_improved = 0\n",
    "## exception\n",
    "training_state = None\n",
    "try:\n",
    "    \n",
    "    ## TODO:  recover training\n",
    "    training_timer.start()\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar_n_epoch.desc = f\"Epoch {epoch} \"\n",
    "        pbar_n_epoch.refresh()\n",
    "        pbar_n_train.n = 0\n",
    "        pbar_n_train.refresh()\n",
    "        pbar_n_valid.n = 0\n",
    "        pbar_n_valid.refresh()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Start training\n",
    "        ## reset variables\n",
    "        epoch_train_log = { \"Train\": \"\", \"epoch\": epoch }\n",
    "        pred_list = []\n",
    "        gt_list = []\n",
    "        accum_batch_loss = 0.0\n",
    "        ## set to training mode\n",
    "        model.train()\n",
    "        for data in train_dataloader:\n",
    "            x_train, y_train, crop_name_batch = data\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device) # move to GPU\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad() # clean gradients before each iteration\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    preds = model(x_train)\n",
    "                    loss_value = loss_fn(preds, y_train)\n",
    "                    \n",
    "                scaler.scale(loss_value).backward() # 計算並縮放損失的梯度\n",
    "                scaler.step(optimizer) # 更新模型參數\n",
    "                scaler.update() # 更新縮放因子\n",
    "                \n",
    "            else:\n",
    "                preds = model(x_train)\n",
    "                loss_value = loss_fn(preds, y_train)\n",
    "                \n",
    "                ## update mode_parameters by back_propagation\n",
    "                loss_value.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            ## extend 'pred_list', 'gt_list'\n",
    "            pred_prob = torch.nn.functional.softmax(preds, dim=1)\n",
    "            _, pred_train = torch.max(pred_prob, 1) # get the highest probability class\n",
    "            pred_list.extend(pred_train.cpu().numpy().tolist()) # conversion flow: Tensor --> ndarray --> list\n",
    "            gt_list.extend(y_train.cpu().numpy().tolist())\n",
    "            ## add current batch loss\n",
    "            accum_batch_loss += loss_value.item() # get value of Tensor\n",
    "            \n",
    "            ## update 'pbar_n_train'\n",
    "            pbar_n_train.update(1)\n",
    "            pbar_n_train.refresh()\n",
    "        \n",
    "        if use_lr_schedular: lr_scheduler.step() # update 'lr' for each epoch\n",
    "        \n",
    "        caulculate_metrics(epoch_train_log, (accum_batch_loss/len(train_dataloader)),\n",
    "                           gt_list, pred_list, class2num_dict)\n",
    "        # print(json.dumps(epoch_train_log, indent=4))\n",
    "        train_logs.append(epoch_train_log)\n",
    "        ## update postfix of 'pbar_n_train'\n",
    "        if use_lr_schedular: pbar_n_train.postfix = f\" {'{'} Loss: {epoch_train_log['average_loss']}, Weighted_f1: {epoch_train_log['weighted_f1']}, lr: {lr_scheduler.get_last_lr()[0]:.0e} {'}'} \"\n",
    "        else: pbar_n_train.postfix = f\" {'{'} Loss: {epoch_train_log['average_loss']}, Weighted_f1: {epoch_train_log['weighted_f1']} {'}'} \"\n",
    "        pbar_n_train.refresh()\n",
    "        # End training\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Start validating\n",
    "        ## reset variables\n",
    "        epoch_valid_log = { \"Valid\": \"\", \"epoch\": epoch }\n",
    "        pred_list = []\n",
    "        gt_list = []\n",
    "        accum_batch_loss = 0.0\n",
    "        ## set to evaluation mode\n",
    "        model.eval() \n",
    "        with torch.no_grad(): \n",
    "            for data in valid_dataloader:\n",
    "                x_valid, y_valid, crop_name_batch = data\n",
    "                x_valid, y_valid = x_valid.to(device), y_valid.to(device) # move to GPU\n",
    "                preds = model(x_valid)\n",
    "                loss_value = loss_fn(preds, y_valid)\n",
    "                \n",
    "                ## extend 'pred_list', 'gt_list'\n",
    "                pred_prob = torch.nn.functional.softmax(preds, dim=1)\n",
    "                _, pred_valid = torch.max(pred_prob, 1)\n",
    "                pred_list.extend(pred_valid.cpu().numpy().tolist())\n",
    "                gt_list.extend(y_valid.cpu().numpy().tolist())\n",
    "                ## add current batch loss\n",
    "                accum_batch_loss += loss_value.item()\n",
    "                \n",
    "                ## update 'pbar_n_valid'\n",
    "                pbar_n_valid.update(1)\n",
    "                pbar_n_valid.refresh()\n",
    "\n",
    "        caulculate_metrics(epoch_valid_log, (accum_batch_loss/len(valid_dataloader)),\n",
    "                           gt_list, pred_list, class2num_dict)\n",
    "        # print(json.dumps(epoch_valid_log, indent=4))\n",
    "        valid_logs.append(epoch_valid_log)\n",
    "        ## update postfix of 'pbar_n_valid'\n",
    "        pbar_n_valid.postfix = f\" {'{'} Loss: {epoch_valid_log['average_loss']}, Weighted_f1: {epoch_valid_log['weighted_f1']} {'}'} \"\n",
    "        pbar_n_valid.refresh()\n",
    "        # End validating\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Check best condition\n",
    "        epoch_valid_log[\"valid_state\"] = \"\"\n",
    "        if epoch_valid_log[\"weighted_f1\"] > best_val_weighted_f1:\n",
    "            best_val_weighted_f1 = epoch_valid_log[\"weighted_f1\"]\n",
    "            ## update 'best_val_log'\n",
    "            caulculate_metrics(best_val_log, (accum_batch_loss/len(valid_dataloader)),\n",
    "                               gt_list, pred_list, class2num_dict)\n",
    "            epoch_valid_log[\"valid_state\"] = \"☆★☆ BEST_VALIDATION ☆★☆\"\n",
    "            tqdm.write((f\"Epoch: {epoch:0{len(str(epochs))}}, \"\n",
    "                        f\"☆★☆ BEST_VALIDATION ☆★☆, \"\n",
    "                        f\"best_val_avg_loss = {best_val_log['average_loss']}, \"\n",
    "                        f\"best_val_weighted_f1 = {best_val_log['weighted_f1']}\"))\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "            best_optimizer_state_dict = copy.deepcopy(optimizer.state_dict())\n",
    "            best_val_log[\"epoch\"] = epoch # put here to make sure all updates of best state has been done.\n",
    "        \n",
    "        # Update figure \n",
    "        plot_training_trend_kwargs = {\n",
    "            \"plt\"        : plt,\n",
    "            \"save_dir\"   : save_dir,\n",
    "            \"loss_key\"   : \"average_loss\",\n",
    "            \"score_key\"  : \"weighted_f1\",\n",
    "            \"train_logs\" : pd.DataFrame(train_logs),\n",
    "            \"valid_logs\" : pd.DataFrame(valid_logs),\n",
    "        }\n",
    "        plot_training_trend(**plot_training_trend_kwargs)\n",
    "        \n",
    "        # Update 'pbar_n_epoch'\n",
    "        pbar_n_epoch.update(1)\n",
    "        pbar_n_epoch.refresh()\n",
    "        \n",
    "        \n",
    "        # Check 'EarlyStop'\n",
    "        epoch_valid_log[\"valid_improve\"] = \"\"\n",
    "        if enable_earlystop:\n",
    "            if epoch_valid_log[\"average_loss\"] < best_val_avg_loss:\n",
    "                best_val_avg_loss = epoch_valid_log[\"average_loss\"]\n",
    "                accum_no_improved = 0\n",
    "            else:\n",
    "                epoch_valid_log[\"valid_improve\"] = \"◎㊣◎ NO_IMPROVED ◎㊣◎\"\n",
    "                accum_no_improved += 1\n",
    "                if accum_no_improved == max_no_improved: sys.exit()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    training_state = \"KeyboardInterrupt\"\n",
    "    tqdm.write(\"KeyboardInterrupt\")\n",
    "    \n",
    "except SystemExit:\n",
    "    training_state = \"EarlyStop\"\n",
    "    tqdm.write(\"EarlyStop, exit training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    training_state = \"ExceptionError\"\n",
    "    tqdm.write(traceback.format_exc())\n",
    "    with open(os.path.normpath(f\"{save_dir}/{{Logs}}_ExceptionError.log\"), mode=\"w\") as f_writer:\n",
    "        f_writer.write(traceback.format_exc())\n",
    "\n",
    "else:\n",
    "    training_state = \"Completed\"\n",
    "    tqdm.write(\"Training Completed\")\n",
    "    \n",
    "finally:\n",
    "\n",
    "    # Close 'progress_bar'\n",
    "    pbar_n_epoch.close()\n",
    "    pbar_n_train.close()\n",
    "    pbar_n_valid.close()\n",
    "    \n",
    "    # Save training consume time\n",
    "    training_timer.stop()\n",
    "    training_timer.calculate_consume_time()\n",
    "    training_timer.save_consume_time(save_dir, desc=\"training time\")\n",
    "    \n",
    "    \n",
    "    if best_val_log[\"epoch\"] > 0: # If `best_val_log[\"epoch\"]` > 0, all of `logs` and `state_dict` are not empty.\n",
    "        \n",
    "        # Save logs (convert to Dataframe)\n",
    "        save_training_logs(save_dir, train_logs, valid_logs, best_val_log)\n",
    "        \n",
    "        # Save model\n",
    "        save_model(\"best\", save_dir, best_model_state_dict, best_optimizer_state_dict, best_val_log)\n",
    "        save_model(\"final\", save_dir, model.state_dict(), optimizer.state_dict(), {\"train\": train_logs, \"valid\": valid_logs})\n",
    "\n",
    "        # Rename 'save_dir'\n",
    "        ## new_name_format = {time_stamp}_{state}_{target_epochs_with_ImgLoadOptions}_{test_f1}\n",
    "        ## state = {EarlyStop, Interrupt, Completed, Tested, etc.}\n",
    "        rename_training_dir(state=training_state, orig_dir_path=save_dir, \n",
    "                            epochs=valid_logs[-1][\"epoch\"], aug_on_fly=aug_on_fly, use_hsv=use_hsv, time_stamp=time_stamp)\n",
    "        \n",
    "    else: # Delete folder if less than one epoch has been completed.\n",
    "        \n",
    "        print(f\"Less than One epoch has been completed, remove directory '{save_dir}' \")\n",
    "        shutil.rmtree(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebrafish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae46fe3be2f97d3a16702042bc6c7abd422dd0bfb5ce5527ad30c3a287e1c756"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
