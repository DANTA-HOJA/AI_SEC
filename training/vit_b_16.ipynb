{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for ```Training```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import traceback\n",
    "import shutil\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import imgaug as ia\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\confocal_microscope\\Desktop\\ZebraFish_AP_POS\\modules\") # add path to scan customized module\n",
    "from logger import init_logger\n",
    "from fileop import create_new_dir\n",
    "from dl_utils import set_gpu, ImgDataset, caulculate_metrics, save_model, plot_training_trend, \\\n",
    "                     compose_transform, calculate_class_weight, get_sortedClassMapper_from_dir, \\\n",
    "                     rename_training_dir, save_training_logs\n",
    "from misc_utils import Timer\n",
    "\n",
    "# print(\"=\"*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_logger = init_logger(r\"Training\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constant path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_dataset_root = r\"C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\"\n",
    "save_dir_root = r\"C:\\Users\\confocal_microscope\\Desktop\\{Test}_Model_history\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 2023-04-04 04:40:14,029 | Training | INFO | Using 'cuda', device_name = 'NVIDIA GeForce RTX 2080 Ti'\n"
     ]
    }
   ],
   "source": [
    "dataset_name = r\"{20230305_NEW_STRUCT}_Academia_Sinica_i409\"\n",
    "dataset_gen_method = \"fish_dataset_horiz_cut_1l2_Mix_AP\"\n",
    "dataset_param_name = \"DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\"\n",
    "cuda_idx = 1\n",
    "label_in_filename = 0\n",
    "train_ratio = 0.8\n",
    "rand_seed = 2022\n",
    "model_name = \"vit_b_16\"\n",
    "pretrain_weights = \"IMAGENET1K_V1\"\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 1e-5\n",
    "use_hsv = False # using 'HSV' when getting images from the 'ImgDataset'\n",
    "aug_on_fly = True # applying augmentation on the fly\n",
    "forcing_balance = False\n",
    "forcing_sample_amount = 2800\n",
    "enable_earlystop = True\n",
    "max_no_improved = 10 # EarlyStop\n",
    "\n",
    "if aug_on_fly and forcing_balance: raise ValueError(\"'aug_on_fly' and 'forcing_balance' can only set one to True at a time\")\n",
    "\n",
    "debug_mode = False # if True, sample 200 images only\n",
    "\n",
    "# Create path var\n",
    "save_dir_model = os.path.join(save_dir_root, model_name)\n",
    "train_selected_dir = os.path.join(ap_dataset_root, dataset_name, dataset_gen_method, dataset_param_name, \"train\", \"selected\")\n",
    "\n",
    "# Set GPU\n",
    "device, device_name = set_gpu(cuda_idx)\n",
    "training_logger.info(f\"Using '{device}', device_name = '{device_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| 2023-04-04 04:40:14,188 | Training | INFO | num2class_list = ['L', 'M', 'S'], class2num_dict = {'L': 0, 'M': 1, 'S': 2}\n",
      "| 2023-04-04 04:40:14,200 | Training | INFO | total = 1980\n",
      "| 2023-04-04 04:40:14,202 | Training | INFO | train_data (1584)\n",
      "| 2023-04-04 04:40:14,203 | Training | INFO | 0 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\S\\S_fish_25_A_selected_2.tiff\n",
      "| 2023-04-04 04:40:14,203 | Training | INFO | 1 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\S\\S_fish_34_A_selected_0.tiff\n",
      "| 2023-04-04 04:40:14,204 | Training | INFO | 2 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\M\\M_fish_8_A_selected_3.tiff\n",
      "| 2023-04-04 04:40:14,204 | Training | INFO | 3 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\L\\L_fish_198_P_selected_0.tiff\n",
      "| 2023-04-04 04:40:14,204 | Training | INFO | 4 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\S\\S_fish_98_P_selected_0.tiff\n",
      "| 2023-04-04 04:40:14,205 | Training | INFO | ※ : applying augmentation on the fly\n",
      "| 2023-04-04 04:40:14,206 | Training | INFO | ※ : total train batches: 50\n",
      "| 2023-04-04 04:40:14,206 | Training | INFO | valid_data (396)\n",
      "| 2023-04-04 04:40:14,207 | Training | INFO | 0 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\L\\L_fish_191_P_selected_1.tiff\n",
      "| 2023-04-04 04:40:14,207 | Training | INFO | 1 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\S\\S_fish_83_P_selected_1.tiff\n",
      "| 2023-04-04 04:40:14,207 | Training | INFO | 2 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\M\\M_fish_148_P_selected_1.tiff\n",
      "| 2023-04-04 04:40:14,208 | Training | INFO | 3 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\M\\M_fish_63_A_selected_3.tiff\n",
      "| 2023-04-04 04:40:14,208 | Training | INFO | 4 : img_path = C:\\Users\\confocal_microscope\\Desktop\\{Test}_DataSet\\{20230305_NEW_STRUCT}_Academia_Sinica_i409\\fish_dataset_horiz_cut_1l2_Mix_AP\\DS_SURF3C_CRPS512_SF14_INT20_DRP100_RS2022\\train\\selected\\S\\S_fish_29_A_selected_0.tiff\n",
      "| 2023-04-04 04:40:14,208 | Training | INFO | ※ : total valid batches: 13\n",
      "| 2023-04-04 04:40:14,209 | Training | INFO | load model using 'torch.hub.load()', model_name: 'vit_b_16', pretrain_weights: 'IMAGENET1K_V1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: 'C:\\Users\\confocal_microscope\\Desktop\\{Test}_Model_history\\vit_b_16\\Training_20230404_04_40_14' is created!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\confocal_microscope/.cache\\torch\\hub\\pytorch_vision_main\n",
      "C:\\Users\\confocal_microscope/.cache\\torch\\hub\\pytorch_vision_main\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861db2ac834c4b1283e43ca09487fbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch :   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0b7630bd504c7382054b4b876ccac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train :   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64031fa83c5e4dc7958a35bd715aeb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Valid :   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.58604, best_val_avg_f1 = 0.71891\n",
      "Epoch: 002, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.43933, best_val_avg_f1 = 0.80105\n",
      "Epoch: 003, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.39075, best_val_avg_f1 = 0.82036\n",
      "Epoch: 005, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.37699, best_val_avg_f1 = 0.82878\n",
      "Epoch: 006, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.30444, best_val_avg_f1 = 0.85792\n",
      "Epoch: 007, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.27361, best_val_avg_f1 = 0.88637\n",
      "Epoch: 009, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.25615, best_val_avg_f1 = 0.89764\n",
      "Epoch: 012, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.19849, best_val_avg_f1 = 0.92512\n",
      "Epoch: 016, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.17788, best_val_avg_f1 = 0.92664\n",
      "Epoch: 017, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.16468, best_val_avg_f1 = 0.93157\n",
      "Epoch: 018, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.16061, best_val_avg_f1 = 0.93192\n",
      "Epoch: 019, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.1483, best_val_avg_f1 = 0.93422\n",
      "Epoch: 020, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.13559, best_val_avg_f1 = 0.93696\n",
      "Epoch: 022, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.1404, best_val_avg_f1 = 0.93698\n",
      "Epoch: 023, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.13351, best_val_avg_f1 = 0.954\n",
      "Epoch: 030, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.13368, best_val_avg_f1 = 0.95422\n",
      "Epoch: 035, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.09466, best_val_avg_f1 = 0.95928\n",
      "Epoch: 037, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.09396, best_val_avg_f1 = 0.96397\n",
      "Epoch: 041, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.06984, best_val_avg_f1 = 0.97123\n",
      "Epoch: 050, ☆★☆ BEST_VALIDATION ☆★☆, best_val_avg_loss = 0.0658, best_val_avg_f1 = 0.97838\n",
      "EarlyStop, exit training\n"
     ]
    }
   ],
   "source": [
    "# Create save model directory\n",
    "time_stamp = datetime.now().strftime('%Y%m%d_%H_%M_%S')\n",
    "save_dir = os.path.join(save_dir_model, f\"Training_{time_stamp}\")\n",
    "create_new_dir(save_dir)\n",
    "\n",
    "\n",
    "# Set 'np.random.seed'\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "\n",
    "# Create transform and Set 'rand_seed' if 'aug_on_fly' is True\n",
    "if aug_on_fly: \n",
    "    ia.seed(rand_seed) # To get consistent augmentations ( 'imgaug' package )\n",
    "    transform = compose_transform()\n",
    "else: transform = None\n",
    "\n",
    "\n",
    "# Scan classes to create 'class_mapper'\n",
    "num2class_list, class2num_dict = get_sortedClassMapper_from_dir(train_selected_dir)\n",
    "training_logger.info(f\"num2class_list = {num2class_list}, class2num_dict = {class2num_dict}\")\n",
    "\n",
    "\n",
    "# Scan tiff\n",
    "dataset_img_dict = { \"all_classes\" : [] }\n",
    "# random sampling from each class with a constant value (forcing balance)\n",
    "if aug_on_fly: dataset_img_dict['all_classes'] = glob(os.path.normpath(f\"{train_selected_dir}/*/*selected*.tiff\"))\n",
    "elif forcing_balance:\n",
    "    for key, value in class2num_dict.items(): # key: class, value: class_idx\n",
    "        dataset_img_dict[key] = glob(os.path.normpath(f\"{train_selected_dir}/{key}/*.tiff\"))\n",
    "        dataset_img_dict[key] = np.random.choice(dataset_img_dict[key], size=forcing_sample_amount, replace=False)\n",
    "        dataset_img_dict['all_classes'].extend(dataset_img_dict[key])\n",
    "else: dataset_img_dict['all_classes'] = glob(os.path.normpath(f\"{train_selected_dir}/*/*.tiff\"))\n",
    "training_logger.info(f\"total = {len(dataset_img_dict['all_classes'])}\")\n",
    "## debug mode: random select 200 images\n",
    "if debug_mode:\n",
    "    dataset_img_dict['all_classes'] = np.random.choice(dataset_img_dict['all_classes'], size=200, replace=False)\n",
    "    training_logger.info(f\"Debug mode, only select first {len(dataset_img_dict['all_classes'])}\")\n",
    "\n",
    "\n",
    "# Split train, valid dataset\n",
    "train_img_list, valid_img_list = train_test_split(dataset_img_dict['all_classes'], random_state=rand_seed, train_size=train_ratio)\n",
    "## save 'training_amount'\n",
    "training_amount = f\"{{ dataset_{len(dataset_img_dict['all_classes'])} }}_{{ train_{len(train_img_list)} }}_{{ valid_{len(valid_img_list)} }}\"\n",
    "with open(os.path.normpath(f\"{save_dir}/{training_amount}\"), mode=\"w\") as f_writer: pass\n",
    "\n",
    "\n",
    "# Create 'train_set', 'train_dataloader'\n",
    "training_logger.info(f\"train_data ({len(train_img_list)})\")\n",
    "[training_logger.info(f\"{i} : img_path = {train_img_list[i]}\") for i in range(5)]\n",
    "train_set = ImgDataset(train_img_list, class_mapper=class2num_dict, label_in_filename=label_in_filename, \n",
    "                       use_hsv=use_hsv, transform=transform, logger=training_logger)\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True) # TODO:  Dataloader shuffle consistency\n",
    "training_logger.info(f\"※ : total train batches: {len(train_dataloader)}\")\n",
    "\n",
    "\n",
    "# Create 'valid_set', 'valid_dataloader'\n",
    "training_logger.info(f\"valid_data ({len(valid_img_list)})\")\n",
    "[training_logger.info(f\"{i} : img_path = {valid_img_list[i]}\") for i in range(5)]\n",
    "valid_set = ImgDataset(valid_img_list, class_mapper=class2num_dict, label_in_filename=label_in_filename, \n",
    "                       use_hsv=use_hsv)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "training_logger.info(f\"※ : total valid batches: {len(valid_dataloader)}\")\n",
    "\n",
    "\n",
    "# Read test ( debug mode only )\n",
    "if debug_mode:\n",
    "    read_test = cv2.imread(train_img_list[-1])\n",
    "    training_logger.info(f\"Read Test: {train_img_list[-1]}\")\n",
    "    cv2.imshow(\"Read Test\", read_test)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Create model\n",
    "training_logger.info(f\"load model using 'torch.hub.load()', model_name: '{model_name}', pretrain_weights: '{pretrain_weights}'\")\n",
    "model = torch.hub.load('pytorch/vision', model_name, weights=pretrain_weights)\n",
    "## modify model structure\n",
    "model.heads.head = nn.Linear(in_features=768, out_features=len(class2num_dict), bias=True)\n",
    "model.to(device)\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# Initial 'loss function' and 'optimizer'\n",
    "if aug_on_fly:\n",
    "    logs_path = os.path.join(ap_dataset_root, dataset_name, dataset_gen_method, \n",
    "                             dataset_param_name, r\"{Logs}_train_selected_summary.log\")\n",
    "    with open(logs_path, 'r') as f_writer: class_counts: Dict[str, int] = json.load(f_writer)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=calculate_class_weight(class_counts)) # apply 'class_weight'\n",
    "else: \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01) # TODO:  use momentum, lr_scheduler\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=0.01, momentum=0.9) # TODO:  use momentum, lr_scheduler\n",
    "\n",
    "\n",
    "# Training\n",
    "## training variables\n",
    "training_timer = Timer()\n",
    "train_logs = []\n",
    "valid_logs = []\n",
    "## progress bar\n",
    "pbar_n_epoch = tqdm(total=epochs, desc=f\"Epoch \")\n",
    "pbar_n_train = tqdm(total=len(train_dataloader), desc=\"Train \")\n",
    "pbar_n_valid = tqdm(total=len(valid_dataloader), desc=\"Valid \")\n",
    "## best validation condition\n",
    "best_val_log = { \"Best\": time_stamp, \"epoch\": 0 }\n",
    "best_val_avg_f1 = 0.0\n",
    "best_val_avg_loss = np.inf\n",
    "best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "best_optimizer_state_dict = copy.deepcopy(optimizer.state_dict())\n",
    "## early stop\n",
    "accum_no_improved = 0\n",
    "## exception\n",
    "training_state = None\n",
    "try:\n",
    "    \n",
    "    ## TODO:  recover training\n",
    "    training_timer.start()\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar_n_epoch.desc = f\"Epoch {epoch} \"\n",
    "        pbar_n_epoch.refresh()\n",
    "        pbar_n_train.n = 0\n",
    "        pbar_n_train.refresh()\n",
    "        pbar_n_valid.n = 0\n",
    "        pbar_n_valid.refresh()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Start training\n",
    "        ## reset variables\n",
    "        epoch_train_log = { \"Train\": \"\", \"epoch\": epoch }\n",
    "        pred_list = []\n",
    "        gt_list = []\n",
    "        accum_batch_loss = 0.0\n",
    "        ## set to training mode\n",
    "        model.train()\n",
    "        for data in train_dataloader:\n",
    "            x_train, y_train, fish_ID_list = data\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device) # move to GPU\n",
    "            preds = model(x_train)\n",
    "            loss_value = loss_fn(preds, y_train)\n",
    "            \n",
    "            ## update mode_parameters by back_propagation\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad() # clean gradients after step\n",
    "            \n",
    "            ## extend 'pred_list', 'gt_list'\n",
    "            _, pred_train = torch.max(preds, 1) # get the highest probability class\n",
    "            pred_list.extend(pred_train.cpu().numpy().tolist()) # conversion flow: Tensor --> ndarray --> list\n",
    "            gt_list.extend(y_train.cpu().numpy().tolist())\n",
    "            ## add current batch loss\n",
    "            accum_batch_loss += loss_value.item() # get value of Tensor\n",
    "            \n",
    "            ## update 'pbar_n_train'\n",
    "            pbar_n_train.update(1)\n",
    "            pbar_n_train.refresh()\n",
    "        \n",
    "        caulculate_metrics(epoch_train_log, (accum_batch_loss/len(train_dataloader)),\n",
    "                        gt_list, pred_list, class2num_dict)\n",
    "        # print(json.dumps(epoch_train_log, indent=4))\n",
    "        train_logs.append(epoch_train_log)\n",
    "        ## update postfix of 'pbar_n_train'\n",
    "        pbar_n_train.postfix = f\" {'{'} Loss: {epoch_train_log['average_loss']}, Avg_f1: {epoch_train_log['average_f1']} {'}'} \"\n",
    "        pbar_n_train.refresh()\n",
    "        # End training\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Start validating\n",
    "        ## reset variables\n",
    "        epoch_valid_log = { \"Valid\": \"\", \"epoch\": epoch }\n",
    "        pred_list = []\n",
    "        gt_list = []\n",
    "        accum_batch_loss = 0.0\n",
    "        ## set to evaluation mode\n",
    "        model.eval() \n",
    "        with torch.no_grad(): \n",
    "            for data in valid_dataloader:\n",
    "                x_valid, y_valid, fish_ID_list = data\n",
    "                x_valid, y_valid = x_valid.to(device), y_valid.to(device) # move to GPU\n",
    "                preds = model(x_valid)\n",
    "                loss_value = loss_fn(preds, y_valid)\n",
    "                \n",
    "                ## extend 'pred_list', 'gt_list'\n",
    "                _, pred_valid = torch.max(preds, 1)\n",
    "                pred_list.extend(pred_valid.cpu().numpy().tolist())\n",
    "                gt_list.extend(y_valid.cpu().numpy().tolist())\n",
    "                ## add current batch loss\n",
    "                accum_batch_loss += loss_value.item()\n",
    "                \n",
    "                ## update 'pbar_n_valid'\n",
    "                pbar_n_valid.update(1)\n",
    "                pbar_n_valid.refresh()\n",
    "\n",
    "        caulculate_metrics(epoch_valid_log, (accum_batch_loss/len(valid_dataloader)),\n",
    "                        gt_list, pred_list, class2num_dict)\n",
    "        # print(json.dumps(epoch_valid_log, indent=4))\n",
    "        valid_logs.append(epoch_valid_log)\n",
    "        ## update postfix of 'pbar_n_valid'\n",
    "        pbar_n_valid.postfix = f\" {'{'} Loss: {epoch_valid_log['average_loss']}, Avg_f1: {epoch_valid_log['average_f1']} {'}'} \"\n",
    "        pbar_n_valid.refresh()\n",
    "        # End validating\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Check best condition, average_f1 = (macro_f1 + micro_f1)/2\n",
    "        if epoch_valid_log[\"average_f1\"] > best_val_avg_f1:\n",
    "            best_val_avg_f1 = epoch_valid_log[\"average_f1\"]\n",
    "            ## update 'best_val_log'\n",
    "            best_val_log[\"epoch\"] = epoch\n",
    "            caulculate_metrics(best_val_log, (accum_batch_loss/len(valid_dataloader)),\n",
    "                            gt_list, pred_list, class2num_dict)\n",
    "            tqdm.write((f\"Epoch: {epoch:0{len(str(epochs))}}, \"\n",
    "                        f\"☆★☆ BEST_VALIDATION ☆★☆, \"\n",
    "                        f\"best_val_avg_loss = {best_val_log['average_loss']}, \"\n",
    "                        f\"best_val_avg_f1 = {best_val_log['average_f1']}\"))\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "            best_optimizer_state_dict = copy.deepcopy(optimizer.state_dict())\n",
    "        \n",
    "        # Update figure \n",
    "        plot_training_trend_kwargs = {\n",
    "            \"plt\"        : plt,\n",
    "            \"save_dir\"   : save_dir,\n",
    "            \"loss_key\"   : \"average_loss\",\n",
    "            \"score_key\"  : \"average_f1\",\n",
    "            \"train_logs\" : pd.DataFrame(train_logs),\n",
    "            \"valid_logs\" : pd.DataFrame(valid_logs),\n",
    "        }\n",
    "        plot_training_trend(**plot_training_trend_kwargs)\n",
    "        \n",
    "        # Update 'pbar_n_epoch'\n",
    "        pbar_n_epoch.update(1)\n",
    "        pbar_n_epoch.refresh()\n",
    "        \n",
    "        \n",
    "        # Check 'EarlyStop'\n",
    "        if enable_earlystop:\n",
    "            if epoch_valid_log[\"average_loss\"] < best_val_avg_loss:\n",
    "                best_val_avg_loss = epoch_valid_log[\"average_loss\"]\n",
    "                accum_no_improved = 0\n",
    "            else: \n",
    "                accum_no_improved += 1\n",
    "                if accum_no_improved == max_no_improved: sys.exit()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    training_state = \"KeyboardInterrupt\"\n",
    "    tqdm.write(\"KeyboardInterrupt\")\n",
    "    \n",
    "except SystemExit:\n",
    "    training_state = \"EarlyStop\"\n",
    "    tqdm.write(\"EarlyStop, exit training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    training_state = \"ExceptionError\"\n",
    "    tqdm.write(traceback.format_exc())\n",
    "    with open(os.path.normpath(f\"{save_dir}/{{Logs}}_ExceptionError.log\"), mode=\"w\") as f_writer:\n",
    "        f_writer.write(traceback.format_exc())\n",
    "\n",
    "else:\n",
    "    training_state = \"Completed\"\n",
    "    tqdm.write(\"Training Completed\")\n",
    "    \n",
    "finally:\n",
    "\n",
    "    # Close 'progress_bar'\n",
    "    pbar_n_epoch.close()\n",
    "    pbar_n_train.close()\n",
    "    pbar_n_valid.close()\n",
    "    \n",
    "    # Save training consume time\n",
    "    training_timer.stop()\n",
    "    training_timer.calculate_consume_time()\n",
    "    training_timer.save_consume_time(save_dir, desc=\"training time\")\n",
    "    \n",
    "    \n",
    "    if valid_logs: # If 'valid_logs' is not empty, then 'train_logs' and 'best_val_log' are also not empty.\n",
    "        \n",
    "        # Save logs (convert to Dataframe)\n",
    "        save_training_logs(save_dir, train_logs, valid_logs, best_val_log)\n",
    "        \n",
    "        # Save model\n",
    "        save_model(\"best\", save_dir, best_model_state_dict, best_optimizer_state_dict, best_val_log)\n",
    "        save_model(\"final\", save_dir, model.state_dict(), optimizer.state_dict(), {\"train\": train_logs, \"valid\": valid_logs})\n",
    "\n",
    "        # Rename 'save_dir'\n",
    "        ## new_name_format = {time_stamp}_{state}_{target_epochs_with_ImgLoadOptions}_{test_f1}\n",
    "        ## state = {EarlyStop, Interrupt, Completed, Tested, etc.}\n",
    "        rename_training_dir(state=training_state, orig_dir_path=save_dir, \n",
    "                            epochs=valid_logs[-1][\"epoch\"], aug_on_fly=aug_on_fly, use_hsv=use_hsv, time_stamp=time_stamp)\n",
    "        \n",
    "    else: # Delete folder if less than one epoch has been completed.\n",
    "        \n",
    "        print(f\"Less than One epoch has been completed, remove directory '{save_dir}' \")\n",
    "        shutil.rmtree(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebrafish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae46fe3be2f97d3a16702042bc6c7abd422dd0bfb5ce5527ad30c3a287e1c756"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
